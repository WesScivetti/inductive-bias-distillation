python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_0 --weight_dir weights/ --log_dir logs/ --formal_train_size 1 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 1.0 --hot_temperature 1.0 --cold_temperature 1.0 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_0 --weight_dir weights/ --log_dir logs/ --formal_train_size 10 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 1.0 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_0 --weight_dir weights/ --log_dir logs/ --formal_train_size 100 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_0 --weight_dir weights/ --log_dir logs/ --formal_train_size 1000 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 5 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_0 --weight_dir weights/ --log_dir logs/ --formal_train_size 10000 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 5 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_1 --weight_dir weights/ --log_dir logs/ --formal_train_size 1 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 1.0 --hot_temperature 1.0 --cold_temperature 1.0 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_1 --weight_dir weights/ --log_dir logs/ --formal_train_size 10 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 1.0 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_1 --weight_dir weights/ --log_dir logs/ --formal_train_size 100 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_1 --weight_dir weights/ --log_dir logs/ --formal_train_size 1000 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 5 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_1 --weight_dir weights/ --log_dir logs/ --formal_train_size 10000 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 5 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_2 --weight_dir weights/ --log_dir logs/ --formal_train_size 1 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 1.0 --hot_temperature 1.0 --cold_temperature 1.0 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_2 --weight_dir weights/ --log_dir logs/ --formal_train_size 10 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 1.0 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_2 --weight_dir weights/ --log_dir logs/ --formal_train_size 100 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_2 --weight_dir weights/ --log_dir logs/ --formal_train_size 1000 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 5 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_2 --weight_dir weights/ --log_dir logs/ --formal_train_size 10000 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 5 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_3 --weight_dir weights/ --log_dir logs/ --formal_train_size 1 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 1.0 --hot_temperature 1.0 --cold_temperature 1.0 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_3 --weight_dir weights/ --log_dir logs/ --formal_train_size 10 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 1.0 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_3 --weight_dir weights/ --log_dir logs/ --formal_train_size 100 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_3 --weight_dir weights/ --log_dir logs/ --formal_train_size 1000 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 5 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_3 --weight_dir weights/ --log_dir logs/ --formal_train_size 10000 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 5 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_4 --weight_dir weights/ --log_dir logs/ --formal_train_size 1 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 1.0 --hot_temperature 1.0 --cold_temperature 1.0 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_4 --weight_dir weights/ --log_dir logs/ --formal_train_size 10 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 1.0 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_4 --weight_dir weights/ --log_dir logs/ --formal_train_size 100 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_4 --weight_dir weights/ --log_dir logs/ --formal_train_size 1000 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 5 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_4 --weight_dir weights/ --log_dir logs/ --formal_train_size 10000 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 5 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_5 --weight_dir weights/ --log_dir logs/ --formal_train_size 1 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 1.0 --hot_temperature 1.0 --cold_temperature 1.0 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_5 --weight_dir weights/ --log_dir logs/ --formal_train_size 10 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 1.0 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_5 --weight_dir weights/ --log_dir logs/ --formal_train_size 100 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_5 --weight_dir weights/ --log_dir logs/ --formal_train_size 1000 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 5 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_5 --weight_dir weights/ --log_dir logs/ --formal_train_size 10000 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 5 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_6 --weight_dir weights/ --log_dir logs/ --formal_train_size 1 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 1.0 --hot_temperature 1.0 --cold_temperature 1.0 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_6 --weight_dir weights/ --log_dir logs/ --formal_train_size 10 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 1.0 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_6 --weight_dir weights/ --log_dir logs/ --formal_train_size 100 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_6 --weight_dir weights/ --log_dir logs/ --formal_train_size 1000 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 5 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_6 --weight_dir weights/ --log_dir logs/ --formal_train_size 10000 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 5 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_7 --weight_dir weights/ --log_dir logs/ --formal_train_size 1 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 1.0 --hot_temperature 1.0 --cold_temperature 1.0 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_7 --weight_dir weights/ --log_dir logs/ --formal_train_size 10 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 1.0 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_7 --weight_dir weights/ --log_dir logs/ --formal_train_size 100 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_7 --weight_dir weights/ --log_dir logs/ --formal_train_size 1000 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 5 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_7 --weight_dir weights/ --log_dir logs/ --formal_train_size 10000 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 5 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_8 --weight_dir weights/ --log_dir logs/ --formal_train_size 1 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 1.0 --hot_temperature 1.0 --cold_temperature 1.0 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_8 --weight_dir weights/ --log_dir logs/ --formal_train_size 10 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 1.0 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_8 --weight_dir weights/ --log_dir logs/ --formal_train_size 100 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_8 --weight_dir weights/ --log_dir logs/ --formal_train_size 1000 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 5 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_8 --weight_dir weights/ --log_dir logs/ --formal_train_size 10000 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 5 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_9 --weight_dir weights/ --log_dir logs/ --formal_train_size 1 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 1.0 --hot_temperature 1.0 --cold_temperature 1.0 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_9 --weight_dir weights/ --log_dir logs/ --formal_train_size 10 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 1.0 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_9 --weight_dir weights/ --log_dir logs/ --formal_train_size 100 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_9 --weight_dir weights/ --log_dir logs/ --formal_train_size 1000 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 5 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_9 --weight_dir weights/ --log_dir logs/ --formal_train_size 10000 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 5 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_10 --weight_dir weights/ --log_dir logs/ --formal_train_size 1 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 1.0 --hot_temperature 1.0 --cold_temperature 1.0 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_10 --weight_dir weights/ --log_dir logs/ --formal_train_size 10 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 1.0 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_10 --weight_dir weights/ --log_dir logs/ --formal_train_size 100 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_10 --weight_dir weights/ --log_dir logs/ --formal_train_size 1000 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 5 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_10 --weight_dir weights/ --log_dir logs/ --formal_train_size 10000 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 5 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_11 --weight_dir weights/ --log_dir logs/ --formal_train_size 1 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 1.0 --hot_temperature 1.0 --cold_temperature 1.0 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_11 --weight_dir weights/ --log_dir logs/ --formal_train_size 10 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 1.0 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_11 --weight_dir weights/ --log_dir logs/ --formal_train_size 100 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_11 --weight_dir weights/ --log_dir logs/ --formal_train_size 1000 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 5 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_11 --weight_dir weights/ --log_dir logs/ --formal_train_size 10000 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 5 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_12 --weight_dir weights/ --log_dir logs/ --formal_train_size 1 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 1.0 --hot_temperature 1.0 --cold_temperature 1.0 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_12 --weight_dir weights/ --log_dir logs/ --formal_train_size 10 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 1.0 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_12 --weight_dir weights/ --log_dir logs/ --formal_train_size 100 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_12 --weight_dir weights/ --log_dir logs/ --formal_train_size 1000 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 5 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_12 --weight_dir weights/ --log_dir logs/ --formal_train_size 10000 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 5 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_13 --weight_dir weights/ --log_dir logs/ --formal_train_size 1 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 1.0 --hot_temperature 1.0 --cold_temperature 1.0 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_13 --weight_dir weights/ --log_dir logs/ --formal_train_size 10 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 1.0 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_13 --weight_dir weights/ --log_dir logs/ --formal_train_size 100 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_13 --weight_dir weights/ --log_dir logs/ --formal_train_size 1000 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 5 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_13 --weight_dir weights/ --log_dir logs/ --formal_train_size 10000 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 5 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_14 --weight_dir weights/ --log_dir logs/ --formal_train_size 1 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 1.0 --hot_temperature 1.0 --cold_temperature 1.0 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_14 --weight_dir weights/ --log_dir logs/ --formal_train_size 10 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 1.0 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_14 --weight_dir weights/ --log_dir logs/ --formal_train_size 100 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_14 --weight_dir weights/ --log_dir logs/ --formal_train_size 1000 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 5 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_14 --weight_dir weights/ --log_dir logs/ --formal_train_size 10000 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 5 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_15 --weight_dir weights/ --log_dir logs/ --formal_train_size 1 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 1.0 --hot_temperature 1.0 --cold_temperature 1.0 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_15 --weight_dir weights/ --log_dir logs/ --formal_train_size 10 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 1.0 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_15 --weight_dir weights/ --log_dir logs/ --formal_train_size 100 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_15 --weight_dir weights/ --log_dir logs/ --formal_train_size 1000 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 5 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_15 --weight_dir weights/ --log_dir logs/ --formal_train_size 10000 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 5 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_16 --weight_dir weights/ --log_dir logs/ --formal_train_size 1 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 1.0 --hot_temperature 1.0 --cold_temperature 1.0 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_16 --weight_dir weights/ --log_dir logs/ --formal_train_size 10 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 1.0 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_16 --weight_dir weights/ --log_dir logs/ --formal_train_size 100 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_16 --weight_dir weights/ --log_dir logs/ --formal_train_size 1000 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 5 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_16 --weight_dir weights/ --log_dir logs/ --formal_train_size 10000 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 5 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_17 --weight_dir weights/ --log_dir logs/ --formal_train_size 1 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 1.0 --hot_temperature 1.0 --cold_temperature 1.0 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_17 --weight_dir weights/ --log_dir logs/ --formal_train_size 10 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 1.0 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_17 --weight_dir weights/ --log_dir logs/ --formal_train_size 100 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_17 --weight_dir weights/ --log_dir logs/ --formal_train_size 1000 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 5 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_17 --weight_dir weights/ --log_dir logs/ --formal_train_size 10000 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 5 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_18 --weight_dir weights/ --log_dir logs/ --formal_train_size 1 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 1.0 --hot_temperature 1.0 --cold_temperature 1.0 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_18 --weight_dir weights/ --log_dir logs/ --formal_train_size 10 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 1.0 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_18 --weight_dir weights/ --log_dir logs/ --formal_train_size 100 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_18 --weight_dir weights/ --log_dir logs/ --formal_train_size 1000 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 5 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_18 --weight_dir weights/ --log_dir logs/ --formal_train_size 10000 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 5 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_19 --weight_dir weights/ --log_dir logs/ --formal_train_size 1 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 1.0 --hot_temperature 1.0 --cold_temperature 1.0 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_19 --weight_dir weights/ --log_dir logs/ --formal_train_size 10 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 1.0 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_19 --weight_dir weights/ --log_dir logs/ --formal_train_size 100 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_19 --weight_dir weights/ --log_dir logs/ --formal_train_size 1000 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 5 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_19 --weight_dir weights/ --log_dir logs/ --formal_train_size 10000 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 5 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_20 --weight_dir weights/ --log_dir logs/ --formal_train_size 1 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 1.0 --hot_temperature 1.0 --cold_temperature 1.0 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_20 --weight_dir weights/ --log_dir logs/ --formal_train_size 10 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 1.0 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_20 --weight_dir weights/ --log_dir logs/ --formal_train_size 100 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_20 --weight_dir weights/ --log_dir logs/ --formal_train_size 1000 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 5 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_20 --weight_dir weights/ --log_dir logs/ --formal_train_size 10000 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 5 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_21 --weight_dir weights/ --log_dir logs/ --formal_train_size 1 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 1.0 --hot_temperature 1.0 --cold_temperature 1.0 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_21 --weight_dir weights/ --log_dir logs/ --formal_train_size 10 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 1.0 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_21 --weight_dir weights/ --log_dir logs/ --formal_train_size 100 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_21 --weight_dir weights/ --log_dir logs/ --formal_train_size 1000 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 5 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_21 --weight_dir weights/ --log_dir logs/ --formal_train_size 10000 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 5 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_22 --weight_dir weights/ --log_dir logs/ --formal_train_size 1 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 1.0 --hot_temperature 1.0 --cold_temperature 1.0 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_22 --weight_dir weights/ --log_dir logs/ --formal_train_size 10 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 1.0 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_22 --weight_dir weights/ --log_dir logs/ --formal_train_size 100 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_22 --weight_dir weights/ --log_dir logs/ --formal_train_size 1000 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 5 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_22 --weight_dir weights/ --log_dir logs/ --formal_train_size 10000 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 5 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_23 --weight_dir weights/ --log_dir logs/ --formal_train_size 1 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 1.0 --hot_temperature 1.0 --cold_temperature 1.0 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_23 --weight_dir weights/ --log_dir logs/ --formal_train_size 10 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 1.0 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_23 --weight_dir weights/ --log_dir logs/ --formal_train_size 100 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_23 --weight_dir weights/ --log_dir logs/ --formal_train_size 1000 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 5 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_23 --weight_dir weights/ --log_dir logs/ --formal_train_size 10000 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 5 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_24 --weight_dir weights/ --log_dir logs/ --formal_train_size 1 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 1.0 --hot_temperature 1.0 --cold_temperature 1.0 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_24 --weight_dir weights/ --log_dir logs/ --formal_train_size 10 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 1.0 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_24 --weight_dir weights/ --log_dir logs/ --formal_train_size 100 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_24 --weight_dir weights/ --log_dir logs/ --formal_train_size 1000 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 5 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_24 --weight_dir weights/ --log_dir logs/ --formal_train_size 10000 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 5 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_25 --weight_dir weights/ --log_dir logs/ --formal_train_size 1 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 1.0 --hot_temperature 1.0 --cold_temperature 1.0 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_25 --weight_dir weights/ --log_dir logs/ --formal_train_size 10 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 1.0 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_25 --weight_dir weights/ --log_dir logs/ --formal_train_size 100 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_25 --weight_dir weights/ --log_dir logs/ --formal_train_size 1000 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 5 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_25 --weight_dir weights/ --log_dir logs/ --formal_train_size 10000 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 5 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_26 --weight_dir weights/ --log_dir logs/ --formal_train_size 1 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 1.0 --hot_temperature 1.0 --cold_temperature 1.0 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_26 --weight_dir weights/ --log_dir logs/ --formal_train_size 10 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 1.0 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_26 --weight_dir weights/ --log_dir logs/ --formal_train_size 100 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_26 --weight_dir weights/ --log_dir logs/ --formal_train_size 1000 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 5 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_26 --weight_dir weights/ --log_dir logs/ --formal_train_size 10000 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 5 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_27 --weight_dir weights/ --log_dir logs/ --formal_train_size 1 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 1.0 --hot_temperature 1.0 --cold_temperature 1.0 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_27 --weight_dir weights/ --log_dir logs/ --formal_train_size 10 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 1.0 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_27 --weight_dir weights/ --log_dir logs/ --formal_train_size 100 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_27 --weight_dir weights/ --log_dir logs/ --formal_train_size 1000 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 5 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_27 --weight_dir weights/ --log_dir logs/ --formal_train_size 10000 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 5 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_28 --weight_dir weights/ --log_dir logs/ --formal_train_size 1 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 1.0 --hot_temperature 1.0 --cold_temperature 1.0 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_28 --weight_dir weights/ --log_dir logs/ --formal_train_size 10 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 1.0 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_28 --weight_dir weights/ --log_dir logs/ --formal_train_size 100 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_28 --weight_dir weights/ --log_dir logs/ --formal_train_size 1000 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 5 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_28 --weight_dir weights/ --log_dir logs/ --formal_train_size 10000 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 5 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_29 --weight_dir weights/ --log_dir logs/ --formal_train_size 1 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 1.0 --hot_temperature 1.0 --cold_temperature 1.0 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_29 --weight_dir weights/ --log_dir logs/ --formal_train_size 10 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 1.0 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_29 --weight_dir weights/ --log_dir logs/ --formal_train_size 100 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_29 --weight_dir weights/ --log_dir logs/ --formal_train_size 1000 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 5 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_29 --weight_dir weights/ --log_dir logs/ --formal_train_size 10000 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 5 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_30 --weight_dir weights/ --log_dir logs/ --formal_train_size 1 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 1.0 --hot_temperature 1.0 --cold_temperature 1.0 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_30 --weight_dir weights/ --log_dir logs/ --formal_train_size 10 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 1.0 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_30 --weight_dir weights/ --log_dir logs/ --formal_train_size 100 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_30 --weight_dir weights/ --log_dir logs/ --formal_train_size 1000 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 5 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_30 --weight_dir weights/ --log_dir logs/ --formal_train_size 10000 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 5 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_31 --weight_dir weights/ --log_dir logs/ --formal_train_size 1 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 1.0 --hot_temperature 1.0 --cold_temperature 1.0 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_31 --weight_dir weights/ --log_dir logs/ --formal_train_size 10 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 1.0 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_31 --weight_dir weights/ --log_dir logs/ --formal_train_size 100 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_31 --weight_dir weights/ --log_dir logs/ --formal_train_size 1000 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 5 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_31 --weight_dir weights/ --log_dir logs/ --formal_train_size 10000 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 5 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_32 --weight_dir weights/ --log_dir logs/ --formal_train_size 1 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 1.0 --hot_temperature 1.0 --cold_temperature 1.0 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_32 --weight_dir weights/ --log_dir logs/ --formal_train_size 10 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 1.0 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_32 --weight_dir weights/ --log_dir logs/ --formal_train_size 100 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_32 --weight_dir weights/ --log_dir logs/ --formal_train_size 1000 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 5 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_32 --weight_dir weights/ --log_dir logs/ --formal_train_size 10000 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 5 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_33 --weight_dir weights/ --log_dir logs/ --formal_train_size 1 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 1.0 --hot_temperature 1.0 --cold_temperature 1.0 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_33 --weight_dir weights/ --log_dir logs/ --formal_train_size 10 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 1.0 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_33 --weight_dir weights/ --log_dir logs/ --formal_train_size 100 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_33 --weight_dir weights/ --log_dir logs/ --formal_train_size 1000 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 5 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_33 --weight_dir weights/ --log_dir logs/ --formal_train_size 10000 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 5 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_34 --weight_dir weights/ --log_dir logs/ --formal_train_size 1 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 1.0 --hot_temperature 1.0 --cold_temperature 1.0 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_34 --weight_dir weights/ --log_dir logs/ --formal_train_size 10 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 1.0 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_34 --weight_dir weights/ --log_dir logs/ --formal_train_size 100 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_34 --weight_dir weights/ --log_dir logs/ --formal_train_size 1000 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 5 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_34 --weight_dir weights/ --log_dir logs/ --formal_train_size 10000 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 5 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_35 --weight_dir weights/ --log_dir logs/ --formal_train_size 1 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 1.0 --hot_temperature 1.0 --cold_temperature 1.0 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_35 --weight_dir weights/ --log_dir logs/ --formal_train_size 10 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 1.0 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_35 --weight_dir weights/ --log_dir logs/ --formal_train_size 100 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_35 --weight_dir weights/ --log_dir logs/ --formal_train_size 1000 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 5 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_35 --weight_dir weights/ --log_dir logs/ --formal_train_size 10000 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 5 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_36 --weight_dir weights/ --log_dir logs/ --formal_train_size 1 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 1.0 --hot_temperature 1.0 --cold_temperature 1.0 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_36 --weight_dir weights/ --log_dir logs/ --formal_train_size 10 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 1.0 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_36 --weight_dir weights/ --log_dir logs/ --formal_train_size 100 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_36 --weight_dir weights/ --log_dir logs/ --formal_train_size 1000 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 5 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_36 --weight_dir weights/ --log_dir logs/ --formal_train_size 10000 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 5 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_37 --weight_dir weights/ --log_dir logs/ --formal_train_size 1 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 1.0 --hot_temperature 1.0 --cold_temperature 1.0 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_37 --weight_dir weights/ --log_dir logs/ --formal_train_size 10 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 1.0 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_37 --weight_dir weights/ --log_dir logs/ --formal_train_size 100 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_37 --weight_dir weights/ --log_dir logs/ --formal_train_size 1000 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 5 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_37 --weight_dir weights/ --log_dir logs/ --formal_train_size 10000 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 5 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_38 --weight_dir weights/ --log_dir logs/ --formal_train_size 1 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 1.0 --hot_temperature 1.0 --cold_temperature 1.0 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_38 --weight_dir weights/ --log_dir logs/ --formal_train_size 10 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 1.0 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_38 --weight_dir weights/ --log_dir logs/ --formal_train_size 100 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_38 --weight_dir weights/ --log_dir logs/ --formal_train_size 1000 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 5 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_38 --weight_dir weights/ --log_dir logs/ --formal_train_size 10000 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 5 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_39 --weight_dir weights/ --log_dir logs/ --formal_train_size 1 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 1.0 --hot_temperature 1.0 --cold_temperature 1.0 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_39 --weight_dir weights/ --log_dir logs/ --formal_train_size 10 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 1.0 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_39 --weight_dir weights/ --log_dir logs/ --formal_train_size 100 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 10 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_39 --weight_dir weights/ --log_dir logs/ --formal_train_size 1000 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 5 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
python meta_train.py --n_meta_train 25000 --n_meta_valid 500 --n_meta_test 1000 --meta_train_batch_size 10 --meta_eval_batch_size 1000 --max_batches_per_language 20 --meta_test_size 1000 --dataset scfg --withheld_languages language_list --architecture LSTM --n_embd 1024 --n_positions 256 --n_layer 2 --dropout 0.1 --n_epochs 1 --eval_every 100 --learning_rate 0.005 --inner_lr 1.0 --multi_step_loss --model_name pseudo_meta_lm_hidden1024_39 --weight_dir weights/ --log_dir logs/ --formal_train_size 10000 --language_list language_list --eval --eval_formal --prec_rec_n_samples 1000000 --sgd_epochs 5 --adam_epochs 5 --eval_suffix for_paper --return_last --top_p 0.99 --hot_temperature 1.0 --cold_temperature 0.5 --adam_lr 5e-4
