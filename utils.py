
import math
import torch
import logging

if torch.cuda.is_available():
    device = torch.device('cuda')
else:
    device = torch.device('cpu')


# Sample batch_size sequences from the LM
# top_p: truncate the distribution to the top p of the probability mass (1.0 means no truncation)
def sample_from_lm(lm, tokenizer, batch_size=1, top_p=1.00, temperature=1.0):
    lm.eval()
    input_ids = tokenizer(["<bos>"]*batch_size, add_special_tokens=False, return_tensors="pt").to(device)["input_ids"]

    text = lm.generate(input_ids, do_sample=True, max_length=105, top_p=top_p, temperature=temperature, top_k=0, early_stopping=True, pad_token_id=tokenizer.pad_token_id, eos_token_id=3)

    text = tokenizer.batch_decode(text, skip_special_tokens=True)

    return text

# Compute precision and recall
def compute_precision_recall(model_most_common, model_all, true_most_common, true_all):

    precision_numerator = 0
    precision_denominator = 0

    # List of common model-generated sequences that are not in the true set
    model_imprecise = []

    for model_token in model_most_common:
        if model_token in true_all:
            precision_numerator += 1
        else:
            model_imprecise.append(model_token)

        precision_denominator += 1
    

    recall_numerator = 0
    recall_denominator = 0

    # List of true common sequences that were not generated by the model
    model_irrecall = []

    for true_token in true_most_common:
        if true_token in model_all:
            recall_numerator += 1
        else:
            model_irrecall.append(true_token)

        recall_denominator += 1

    precision = precision_numerator * 1.0 / precision_denominator
    recall = recall_numerator * 1.0 / recall_denominator

    if precision + recall == 0:
        fscore = 0
    else:
        fscore = 2 * precision * recall / (precision + recall)

    return precision, recall, fscore, model_imprecise, model_irrecall



# Input: List of lists
# Output: List of lists that are all the same length (the length of the longest list in the input, or
#         potentially the length of context_size, which is what it will be truncated to if longer than that)
def pad_batch(batch, padding_token=None, context_size=None):

    length_to_pad_to = max([len(x) for x in batch])

    if length_to_pad_to > context_size:
        length_to_pad_to = context_size

    new_batch = []

    for elt in batch:
        new_elt = elt[:]

        if len(new_elt) > context_size:
            new_elt = new_elt[:context_size]

        num_pad_tokens_needed = length_to_pad_to - len(new_elt)
        pad_tokens = [padding_token for _ in range(num_pad_tokens_needed)]

        new_elt = new_elt + pad_tokens

        new_batch.append(new_elt)

    return new_batch


def trim_pad_columns(batch, pad_token_id):
    non_empty_mask = (batch["input_ids"] - pad_token_id).abs().sum(dim=0).bool()
    return {"input_ids" : batch["input_ids"][:, non_empty_mask], "labels" : batch["labels"][:, non_empty_mask]}


def meta_mini_batches_from_batch(batch, train_batch_size, test_batch_size, pad_token_id):
    train_mini_batches = []
    test_mini_batches = []

    n_train_mini_batches = math.ceil(len(batch["input_ids"])*1.0 / train_batch_size)
    n_test_mini_batches = math.ceil(len(batch["test_input_ids"])*1.0 / test_batch_size)

    for train_mini_batch_index in range(n_train_mini_batches):
        train_start_index = train_mini_batch_index*train_batch_size
        train_end_index = (train_mini_batch_index+1)*train_batch_size

        train_mini_batch = {"input_ids" : batch["input_ids"][train_start_index:train_end_index], "labels" : batch["labels"][train_start_index:train_end_index]}
        train_mini_batch = trim_pad_columns(train_mini_batch, pad_token_id)
        train_mini_batches.append(train_mini_batch)


    for test_mini_batch_index in range(n_test_mini_batches):
        test_start_index = test_mini_batch_index*test_batch_size
        test_end_index = (test_mini_batch_index+1)*test_batch_size

        test_mini_batch = {"input_ids" : batch["test_input_ids"][test_start_index:test_end_index], "labels" : batch["test_labels"][test_start_index:test_end_index]}
        test_mini_batch = trim_pad_columns(test_mini_batch, pad_token_id)
        test_mini_batches.append(test_mini_batch)

    return train_mini_batches, test_mini_batches





################################################################################################
# Functions to assist with withholding languages from meta-training based on F-score
################################################################################################


# Produce a list of languages to be withheld from meta-training, to ensure
# that we don't meta-train on a meta-evaluation language
# For each language, produce: 
# - Dictionary of all strings
# - List of top 25
# Then, datasets is a list of 2-tuples, containing
# those 2 elements for each language
# filename is a file listing the languages to be withheld
def load_withheld_languages(filename):

    datasets = []

    # For short-circuiting: Don't have to compute F-score
    # across all datasets if there are unattested strings in
    # the proposed language
    all_seq_dict = {}

    fi = open("formal_languages/" + filename + ".txt", "r")
    
    alphabet = {}
    alphabet["a"] = "0"
    alphabet["b"] = "1"
    alphabet["c"] = "2"
    alphabet["d"] = "3"
    alphabet["e"] = "4"

    alphabet["("] = "0"
    alphabet[")"] = "1"

    for line in fi:
        parts = line.strip().split("\t")
        if len(parts) != 2:
            continue

        language_name = parts[0]

        data_file = open("formal_languages/Fleet/Models/FormalLanguageTheory-Complex/data/" + language_name + ".txt")
        data_list = []
        data_dict = {}
            
        for data_line in data_file:
            parts = data_line.strip().split("\t")
            char_seq = parts[0]
            count = int(parts[1])

            int_seq = []
            for char in char_seq:
                int_seq.append(alphabet[char])
            int_seq = " ".join(int_seq)

            data_dict[int_seq] = 1
            data_list.append((int_seq, count))

            all_seq_dict[int_seq] = 1

        # Get top 25, for F-score
        sorted_data_list = sorted(data_list, key=lambda x: x[1])[::-1]
        top_25 = sorted_data_list[:25]
        top_25 = [x[0] for x in top_25]
        datasets.append([top_25, data_dict])

    return datasets, all_seq_dict


# Compute the F-score between 2 languages
# If the F-score is 1, we conclude they are the same language, and withhold
# the candidate language that is being checked from training
def fscore_for_withholding(language1_top25, language1_dict, language2_top25, language2_dict):
    precision_numerator = 0
    precision_denominator = 0

    recall_numerator = 0
    recall_denominator = 0

    for elt in language1_top25:
        precision_denominator += 1
        if elt in language2_dict:
            precision_numerator += 1

    for elt in language2_top25:
        recall_denominator += 1
        if elt in language1_dict:
            recall_numerator += 1

    if precision_denominator == 0:
        precision = 0
    else:
        precision = precision_numerator * 1.0 / precision_denominator

    if recall_denominator == 0:
        recall = 0
    else:
        recall = recall_numerator * 1.0 / recall_denominator

    if precision+recall == 0:
        fscore = 0
    else:
        fscore = 2*precision*recall / (precision+recall)
    
    return fscore


# Returns true if we should withhold this language, false otherwise
def withhold_based_on_fscore(language_counter, dataset_list, dataset, all_seq_dict):

    # Check for short-circuit option: If the proposed dataset
    # contains any sequence that's not in any of the languages
    # to withhold from, then the proposed dataset doesn't need
    # to be withheld
    for seq in dataset:
        if seq not in all_seq_dict:
            return False

    language_top_25 = language_counter.most_common(25)
    language_top_25 = [x[0] for x in language_top_25]

    for internal_dataset in dataset_list:

        fscore = fscore_for_withholding(language_top_25, language_counter, internal_dataset[0], internal_dataset[1])

        if fscore == 1.0:
            return True

    return False


