
import logging


################################################################################################
# Evaluations for language models (next-word prediction models)
################################################################################################

def minimal_pair(model, dataset, unacceptable_sentence, acceptable_sentence):

    model.eval()

    inp_acc = dataset.tokenizer([acceptable_sentence], add_special_tokens=False)
    inp_unacc = dataset.tokenizer([unacceptable_sentence], add_special_tokens=False)

    inp_acc = dataset.prepare_input({"input_ids" : inp_acc["input_ids"], "labels" : inp_acc["input_ids"]})
    inp_unacc = dataset.prepare_input({"input_ids" : inp_unacc["input_ids"], "labels" : inp_unacc["input_ids"]})


    outp_acc = model(inp_acc)
    outp_unacc = model(inp_unacc)

    perplexity_acc = outp_acc["loss"]
    perplexity_unacc = outp_unacc["loss"]

    # "True" means "got the minimal pair correct"
    if perplexity_acc < perplexity_unacc:
        return True
    else:
        return False

# Logprob that the sequence would have under a unigram model
def random_logprob(sequence, vocab_size):

    # Include EOS token
    vocab_size = vocab_size + 1

    # Include EOS token
    length = len(sequence.split()) + 1

    logprob = -1.0*length*math.log(vocab_size)

    return logprob

# Logprob that the sequence has under a model
def model_logprob(sequence, model):

    sequence = "<bos> " + sequence + " <ENDTOKEN>"
    inp = model.tokenizer([sequence], add_special_tokens=False)
    for key in inp:
        inp[key] = torch.LongTensor(inp[key]).to(device)
    inp["labels"] = inp["input_ids"].clone()

    outp = model(inp)
    loss = outp["loss"] * (len(sequence.split()) - 1)

    return -1*loss



def lm_precision_recall(model, dataset, top_p=1.00, n_samples=10000, sgd_lr=None, sgd_epochs=1, adam_epochs=100, return_last=False, random_normalized=False):

    # Counters for the strings generated by each model or language
    lm_counter = Counter()
    hyp_counter = Counter()
    memorization_counter = Counter()

    temp_model = copy.deepcopy(model)

    if opt_type == "SGD":
        new_opt = torch.optim.SGD(temp_model.parameters(), lr=sgd_lr)
    else:
        new_opt = torch.optim.AdamW(temp_model.parameters(), lr=5e-4)

    train = {"input_ids" : dataset["input_ids"], "labels" : dataset["labels"]}

    for seq in dataset["input_ids"]:
        elt = model.tokenizer.decode(seq, skip_special_tokens=True)
        memorization_counter[elt] += 1

    temp_model = simple_train_model(temp_model, dataset, sgd_lr=sgd_lr, sgd_epochs=sgd_epochs, adam_epochs=adam_epochs, return_last=return_last)

    all_strings = dataset["all_strings"]

    for seq, count in all_strings:
        hyp_counter[seq] = count

    for _ in range(n_samples // 1000):
        lm_text = sample_from_lm(temp_model, model.tokenizer, batch_size=1000, top_p=top_p)
        lm_counter.update(lm_text)

    
    # Get the 25 most common tokens from the various generators
    lm_most_common = lm_counter.most_common(25)
    logging.info("LM most common: " + str(lm_most_common))

    if random_normalized:

        lm_scored = []

        for sequence in lm_counter:
            lm_scored.append((sequence, (model_logprob(sequence, temp_model) - random_logprob(sequence, 2)).item()))

        lm_most_common = sorted(lm_scored, key=lambda x: -1*x[1])[:25]

    hyp_most_common = hyp_counter.most_common(25)
    memorization_most_common = memorization_counter.most_common(25)

    logging.info("LM most common: " + str(lm_most_common))

    lm_most_common = [x[0] for x in lm_most_common]
    hyp_most_common = [x[0] for x in hyp_most_common]
    memorization_most_common = [x[0] for x in memorization_most_common]


    # Compute precision and recall for LM and memorization
    lm_precision, lm_recall, lm_fscore, model_imprecise, model_irrecall = compute_precision_recall(lm_most_common, lm_counter, hyp_most_common, hyp_counter)
    memorization_precision, memorization_recall, memorization_fscore, _, _ = compute_precision_recall(memorization_most_common, memorization_counter, hyp_most_common, hyp_counter)


    logging.info("LM-generated common sequences that are ungrammatical:")
    logging.info(model_imprecise)
    logging.info("Grammatical sequences that the model is missing:")
    logging.info(model_irrecall)

    return lm_precision, lm_recall, lm_fscore, memorization_precision, memorization_recall, memorization_fscore



def zorro_eval(model, dataset):
    zorro_categories = ["agreement_determiner_noun-across_1_adjective",
            "agreement_determiner_noun-between_neighbors",
            "agreement_subject_verb-across_prepositional_phrase",
            "agreement_subject_verb-across_relative_clause",
            "agreement_subject_verb-in_question_with_aux",
            "agreement_subject_verb-in_simple_question",
            "anaphor_agreement-pronoun_gender",
            "argument_structure-dropped_argument",
            "argument_structure-swapped_arguments",
            "argument_structure-transitive",
            "binding-principle_a",
            "case-subjective_pronoun",
            "ellipsis-n_bar",
            "filler-gap-wh_question_object",
            "filler-gap-wh_question_subject",
            "irregular-verb",
            "island-effects-adjunct_island",
            "island-effects-coordinate_structure_constraint",
            "local_attractor-in_question_with_aux",
            "npi_licensing-matrix_question",
            "npi_licensing-only_npi_licensor",
            "quantifiers-existential_there",
            "quantifiers-superlative"
            ]

    total_correct = 0
    total_total = 0

    for category in zorro_categories:
        fi = open("Zorro/sentences/babyberta/" + category + ".txt", "r")

        pairs = []
        current_pair = []

        for line in fi:
            current_pair.append(line.strip())
            if len(current_pair) == 2:
                pairs.append(current_pair)
                current_pair = []

        correct = 0
        total = 0
        for pair in pairs:
            if minimal_pair(model, dataset, pair[0], pair[1]):
                correct += 1

            total += 1

        total_correct += correct
        total_total += total

        logging.info("MINIMAL PAIR RESULTS:" + category + " " + str(correct) + " " + str(total) + " " + str(correct*1.0/total))

    logging.info("MINIMAL PAIR RESULTS: OVERALL: " + str(total_correct) + " " + str(total_total) + " " + str(total_correct*1.0/total_total))

def agr_eval(model, dataset):
    agr_categories = [
                      "copy_minimal_pair",
    #                  "agr_0distractors",
    #                  "agr_1distractors",
    #                  "agr_2distractors",
    #                  "agr_3distractors"
                     ]

    total_correct = 0
    total_total = 0

    for category in agr_categories:
        fi = open("targeted_evals/" + category + ".txt", "r")

        pairs = []
        current_pair = []

        for line in fi:
            current_pair.append(line.strip())
            if len(current_pair) == 2:
                pairs.append(current_pair)
                current_pair = []

        correct = 0
        total = 0
        for pair in pairs:
            if minimal_pair(model, dataset, pair[0], pair[1]):
                correct += 1

            total += 1

        total_correct += correct
        total_total += total

        logging.info("MINIMAL PAIR RESULTS:" + category + " " + str(correct) + " " + str(total) + " " + str(correct*1.0/total))

    logging.info("MINIMAL PAIR RESULTS: OVERALL: " + str(total_correct) + " " + str(total_total) + " " + str(total_correct*1.0/total_total))



def sample_generation(dataset, trainer):

    input_ids = dataset.tokenizer(["the"], add_special_tokens=False, return_tensors="pt").to(device)["input_ids"]

    # Pure sampling
    text = trainer.model.generate(input_ids, do_sample=True, max_length=min(args.n_positions-10,100), top_p=1.00, top_k=0, early_stopping=True, pad_token_id=dataset.tokenizer.pad_token_id, eos_token_id=-1)
    text = dataset.tokenizer.decode(text[0], skip_special_tokens=True)
    logging.info("Pure sampling:")
    logging.info(text)

    # Top-40
    text = trainer.model.generate(input_ids, do_sample=True, max_length=min(args.n_positions-10,100), top_p=1.00, top_k=40, early_stopping=True, pad_token_id=dataset.tokenizer.pad_token_id, eos_token_id=-1)
    text = dataset.tokenizer.decode(text[0], skip_special_tokens=True)
    logging.info("Top-40:")
    logging.info(text)


def perplexity_difference(model1, model2, dataset):
    
    for batch in dataset:
        loss1 = model1(batch, per_token_loss=True)["loss"].tolist()
        loss2 = model1(batch, per_token_loss=True)["loss"].tolist()

        print(batch)
        print(loss1)
        print(loss2)
        tokens_list = [token for sublist in batch["input_ids"].tolist() for token in dataset.tokenizer.decode(sublist[1:], clean_up_tokenization_spaces=False).split()]
        #tokens_list = [dataset.tokenizer.decode(x) for x in [token for sublist in batch["input_ids"].tolist() for token in sublist[1:]]]
        print(len(loss1))
        print(len(loss2))
        print(len(tokens_list))

        three_tuples = []
        for token_index in range(len(loss1)):
            context = " ".join(tokens_list[max(0, token_index-10):token_index+1])
            three_tuple = [context, loss1[token_index], loss2[token_index]]
            three_tuples.append(three_tuple)

        three_tuples = sorted(three_tuples, key=lambda x: x[1] / x[2])
        print(model1.name + " is better:")
        for elt in three_tuples[:100]:
            print(elt)
        print("")

        print(model2.name + " is better:")
        for elt in three_tuples[::-1][:100]:
            print(elt)
        #print(dataset.tokenizer.decode([token for sublist in batch["input_ids"].tolist() for token in sublist]))
        15/0


################################################################################################
# Evaluations for meta language models
################################################################################################


def eval_formal(model, language_list, formal_train_size=None, formal_test_size=None, meta_train_batch_size=None, n_positions=None, top_p=None, prec_rec_n_samples=None, inner_lr=None, sgd_epochs=None, adam_epochs=None, return_last=None, random_normalized=None):
    if language_list == "language_list":
        n_langs = 56
    elif language_list == "language_list_simplified":
        n_langs = 20

    yandp_dataset = formal_dataset(language_list, training_size=formal_train_size, test_size=formal_test_size, batch_size=meta_train_batch_size)
    
    yandp_meta_dataset = MetaLMDataset(create_dataset=yandp_dataset, meta_train_size=0, meta_valid_size=0, meta_test_size=n_langs, integer_vocab_size=integer_vocab_size, context_size=n_positions)

    total_lm_precision = 0
    total_lm_recall = 0
    total_lm_fscore = 0

    total_mem_precision = 0
    total_mem_recall = 0
    total_mem_fscore = 0

    denom = 0


    for inputs in yandp_meta_dataset.test:
        logging.info("Language: " + inputs["name"])
        logging.info("Description: " + inputs["description"])
        lm_precision, lm_recall, lm_fscore, mem_precision, mem_recall, mem_fscore = lm_precision_recall(model, inputs, top_p=top_p, n_samples=prec_rec_n_samples, sgd_lr=inner_lr, sgd_epochs=sgd_epochs, adam_epochs=adam_epochs, return_last=return_last, random_normalized=random_normalized)

        total_lm_precision += lm_precision
        total_lm_recall += lm_recall
        total_lm_fscore += lm_fscore

        total_mem_precision += mem_precision
        total_mem_recall += mem_recall
        total_mem_fscore += mem_fscore

        denom += 1

        logging.info("LM precision, recall, fscore: " + str(lm_precision) + " " + str(lm_recall) + " " + str(lm_fscore))
        logging.info("Memorization precision, recall, fscore: " + str(mem_precision) + " " + str(mem_recall) + " " + str(mem_fscore))
        logging.info("")

    logging.info("Average LM Y&P precision: " + str(total_lm_precision / denom))
    logging.info("Average LM Y&P recall: " + str(total_lm_recall / denom))
    logging.info("Average LM Y&P F-score: " + str(total_lm_fscore / denom))
    logging.info("")
    logging.info("Average memorization Y&P precision: " + str(total_mem_precision / denom))
    logging.info("Average memorization Y&P recall: " + str(total_mem_recall / denom))
    logging.info("Average memorization Y&P F-score: " + str(total_mem_fscore / denom))
    logging.info("")













